# Deep Learning Foundations

## Overview

This repository contains a comprehensive study of deep learning fundamentals, covering tensor operations, mathematical foundations, and neural network implementations across multiple frameworks. Each notebook has been executed with complete outputs, and detailed explanatory videos document the learning process.

## Course Information

**Course:** Deep Learning 258  
**Focus:** Fundamental understanding of deep learning tensor operations and basic network architectures  
**Objective:** Master the foundational concepts required for advanced deep learning work

## Repository Structure

deep-learning-foundations/
├── notebooks/           # Executed Jupyter notebooks with outputs
├── documentation/       # Additional notes and resources
└── README.md           # This file


## Contents

### 1. Neural Network Playground
**Topic:** Interactive visualization of neural network learning  
**Colab:** N/A (Web-based tool)  
**Website:** https://dlmastery.github.io/neural-network-playground/  
**Video:** [Add YouTube link]  
**Description:** Exploration of how neural networks learn through interactive visualization, covering dataset selection, architecture design, and hyperparameter tuning.

### 2. Introduction to NumPy
**Topic:** Foundational numerical computing library  
**Colab:** [Add Google Colab link]  
**GitHub:** [Add .ipynb link]  
**Video:** [Add YouTube link]  
**Description:** Core NumPy operations including array creation, manipulation, broadcasting, and linear algebra operations essential for deep learning.

### 3. Introduction to PyTorch Tensors
**Topic:** PyTorch's core data structure and automatic differentiation  
**Colab:** [Add Google Colab link]  
**GitHub:** [Add .ipynb link]  
**Video:** [Add YouTube link]  
**Description:** PyTorch tensor operations, automatic differentiation with autograd, and GPU acceleration fundamentals.

### 4. Introduction to TensorFlow Tensors
**Topic:** TensorFlow tensor operations and GradientTape  
**Colab:** [Add Google Colab link]  
**GitHub:** [Add .ipynb link]  
**Video:** [Add YouTube link]  
**Description:** TensorFlow tensor manipulation, automatic differentiation with GradientTape, and eager execution.

### 5. Introduction to JAX Tensors
**Topic:** JAX arrays and functional programming for deep learning  
**Colab:** [Add Google Colab link]  
**GitHub:** [Add .ipynb link]  
**Video:** [Add YouTube link]  
**Description:** JAX's NumPy-like API, functional transformations, JIT compilation, and automatic differentiation.

### 6. Calculus for Deep Learning
**Topic:** Mathematical foundations of gradient-based optimization  
**Colab:** [Add Google Colab link]  
**GitHub:** [Add .ipynb link]  
**Video:** [Add YouTube link]  
**Description:** Derivatives, partial derivatives, chain rule, and backpropagation mathematics.

### 7. Probability for Deep Learning
**Topic:** Statistical foundations for neural networks  
**Colab:** [Add Google Colab link]  
**GitHub:** [Add .ipynb link]  
**Video:** [Add YouTube link]  
**Description:** Probability distributions, expected value, variance, maximum likelihood estimation, and stochastic gradient descent.

### 8. Linear Algebra for Deep Learning
**Topic:** Matrix operations powering neural networks  
**Colab:** [Add Google Colab link]  
**GitHub:** [Add .ipynb link]  
**Video:** [Add YouTube link]  
**Description:** Vectors, matrices, matrix multiplication, transpose operations, and their application in neural network layers.

### 9. Neural Networks from Scratch
**Topic:** Building neural networks using only NumPy  
**Colab:** [Add Google Colab link]  
**GitHub:** [Add .ipynb link]  
**Video:** [Add YouTube link]  
**Description:** Implementation of forward propagation, backpropagation, and gradient descent without deep learning frameworks.

### 10. Why Neural Networks Work - Geometric Intuition
**Topic:** Visualizing neural network transformations  
**Colab:** [Add Google Colab link]  
**GitHub:** [Add .ipynb link]  
**Video:** [Add YouTube link]  
**Description:** Geometric interpretation of how neural networks transform data to make it linearly separable.

### 11. From NumPy to PyTorch
**Topic:** Transitioning from pure NumPy to PyTorch  
**Colab:** [Add Google Colab link]  
**GitHub:** [Add .ipynb link]  
**Video:** [Add YouTube link]  
**Description:** Complete journey from manual implementation to PyTorch's automatic differentiation, optimizers, and built-in layers.

### 12. Advanced PyTorch
**Topic:** Production-ready PyTorch techniques  
**Colab:** [Add Google Colab link]  
**GitHub:** [Add .ipynb link]  
**Video:** [Add YouTube link]  
**Description:** Custom layers, transfer learning, learning rate scheduling, mixed precision training, and model deployment.

### 13. From NumPy to TensorFlow & Keras
**Topic:** Complete deep learning journey with TensorFlow  
**Colab:** [Add Google Colab link]  
**GitHub:** [Add .ipynb link]  
**Video:** [Add YouTube link]  
**Description:** Keras Sequential and Functional APIs, model compilation, training with model.fit(), and TensorFlow ecosystem.

### 14. Advanced TensorFlow
**Topic:** Production-ready TensorFlow techniques  
**Colab:** [Add Google Colab link]  
**GitHub:** [Add .ipynb link]  
**Video:** [Add YouTube link]  
**Description:** Custom layers and models, tf.function optimization, distribution strategies, and TensorFlow Extended (TFX).

### 15. JAX Neural Networks
**Topic:** Functional programming approach to deep learning  
**Colab:** [Add Google Colab link]  
**GitHub:** [Add .ipynb link]  
**Video:** [Add YouTube link]  
**Description:** Building neural networks with JAX's functional paradigm, JIT compilation, and vmap vectorization.

## Key Learnings

This comprehensive study provided deep understanding of:

- Tensor operations across NumPy, PyTorch, TensorFlow, and JAX
- Mathematical foundations essential for deep learning (calculus, probability, linear algebra)
- Neural network architectures and their geometric intuition
- Implementation details from scratch to production-ready frameworks
- Framework-specific advantages and use cases
- Best practices for training, optimization, and deployment

## Technical Stack

**Languages:** Python  
**Libraries:** NumPy, PyTorch, TensorFlow, Keras, JAX  
**Tools:** Google Colab, Jupyter Notebook, Git  
**Concepts:** Automatic differentiation, backpropagation, gradient descent, neural network architectures

## Execution Notes

All notebooks have been executed with complete outputs visible. Each implementation has been tested and verified to run successfully. Video explanations provide detailed code walkthroughs and concept explanations.

## Resources and References

**Documentation:**
- PyTorch: https://pytorch.org/docs
- TensorFlow: https://tensorflow.org/guide
- JAX: https://jax.readthedocs.io
- NumPy: https://numpy.org/doc

**Additional Resources:**
- Neural Network Playground: https://dlmastery.github.io/neural-network-playground/
- Course materials and lecture notes
- Research papers and tutorials referenced in notebooks

## License

This repository is for educational purposes as part of Deep Learning 258 coursework.

## Acknowledgments

Thanks to the course instructors and the open-source community for providing excellent learning resources and frameworks.

---

**Note:** All Google Colab notebooks are shared with "Anyone with the link can view" permissions for grading purposes.
